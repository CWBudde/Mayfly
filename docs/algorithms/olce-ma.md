# OLCE-MA - Orthogonal Learning and Chaotic Exploitation Mayfly Algorithm

## Research Reference

**Zhou, D., Kang, Z., Su, X., & Yang, C. (2022). An enhanced Mayfly optimization algorithm based on orthogonal learning and chaotic exploitation strategy. International Journal of Machine Learning and Cybernetics, 13, 3625â€“3643.**

https://doi.org/10.1007/s13042-022-01617-4

## Overview

OLCE-MA enhances the standard algorithm with orthogonal experimental design and chaotic perturbations. This variant excels on complex multimodal optimization problems by improving population diversity and local search capability.

## Key Innovations

### 1. Orthogonal Learning

Applies **orthogonal experimental design** to elite males (top 20% of population):

- **Purpose**: Increase diversity and reduce oscillatory movement
- **Method**: Systematic exploration of parameter combinations
- **Effect**: More efficient search of the solution space
- **Applied to**: Elite males after sorting by fitness

**Benefits**:
- Explores parameter space more systematically than random search
- Reduces redundant evaluations
- Maintains population diversity in promising regions

### 2. Chaotic Exploitation

Uses a **logistic chaotic map** to perturb offspring positions:

```
x_chaos = ChaosFactor * chaos_value * (UpperBound - LowerBound)
```

Where chaos values follow the logistic map:
```
z(n+1) = 4 * z(n) * (1 - z(n))
```

**Properties**:
- Deterministic but appears random
- Covers search space ergodically
- Helps escape local optima
- Improves local search capability

### 3. Adaptive Strategy

The algorithm balances exploration and exploitation through proven parameter defaults that work well without tuning.

## Usage Examples

### Basic Usage

```go
package main

import (
    "fmt"
    "github.com/cwbudde/mayfly"
)

func main() {
    // Use OLCE-MA for multimodal problems
    config := mayfly.NewOLCEConfig()
    config.ObjectiveFunc = mayfly.Rastrigin  // Highly multimodal function
    config.ProblemSize = 10
    config.LowerBound = -10
    config.UpperBound = 10
    config.MaxIterations = 500

    result, err := mayfly.Optimize(config)
    if err != nil {
        panic(err)
    }

    fmt.Printf("Best Cost: %f\n", result.GlobalBest.Cost)
}
```

### Advanced Usage with Custom Parameters

```go
package main

import (
    "fmt"
    "math/rand"
    "github.com/cwbudde/mayfly"
)

func main() {
    // Configure OLCE-MA for high-dimensional multimodal optimization
    config := mayfly.NewOLCEConfig()
    config.ObjectiveFunc = mayfly.Rastrigin
    config.ProblemSize = 30  // High dimensionality
    config.LowerBound = -5.12
    config.UpperBound = 5.12
    config.MaxIterations = 1000

    // Tune OLCE-specific parameters
    config.OrthogonalFactor = 0.4  // Increase exploration
    config.ChaosFactor = 0.15       // Stronger chaos perturbation

    // Increase population for high-D problems
    config.NPop = 40
    config.NPopF = 40

    // Use fixed seed for reproducibility
    config.Rand = rand.New(rand.NewSource(42))

    result, err := mayfly.Optimize(config)
    if err != nil {
        panic(err)
    }

    fmt.Printf("Final Cost: %.6f\n", result.GlobalBest.Cost)
    fmt.Printf("Iterations: %d\n", result.IterationCount)
    fmt.Printf("Function Evaluations: %d\n", result.FuncEvalCount)
}
```

### Real-World Example: Neural Network Hyperparameter Tuning

```go
package main

import (
    "fmt"
    "math"
    "github.com/cwbudde/mayfly"
)

// Simulate neural network validation error for given hyperparameters
func nnValidationError(params []float64) float64 {
    learningRate := params[0]
    momentum := params[1]
    dropout := params[2]
    l2Reg := params[3]

    // Simulate training with these hyperparameters
    // (replace with actual model training in practice)

    // Penalize extreme values
    error := 0.0

    // Learning rate: optimal around 0.001-0.01
    lrOptimal := 0.005
    error += math.Abs(learningRate - lrOptimal) * 10

    // Momentum: optimal around 0.9
    error += math.Abs(momentum - 0.9) * 5

    // Dropout: optimal around 0.2-0.3
    dropoutOptimal := 0.25
    error += math.Abs(dropout - dropoutOptimal) * 8

    // L2 regularization: optimal around 0.0001
    l2Optimal := 0.0001
    error += math.Abs(l2Reg - l2Optimal) * 1000

    // Add some noise to simulate stochastic training
    noise := (math.Sin(learningRate*100) + math.Cos(momentum*50)) * 0.1

    return error + math.Abs(noise)
}

func main() {
    fmt.Println("=== Neural Network Hyperparameter Optimization with OLCE-MA ===\n")

    // OLCE-MA excels at this multimodal optimization problem
    config := mayfly.NewOLCEConfig()
    config.ObjectiveFunc = nnValidationError
    config.ProblemSize = 4  // 4 hyperparameters
    config.LowerBound = 0.0001
    config.UpperBound = 0.5
    config.MaxIterations = 300

    // Smaller population for expensive evaluations
    config.NPop = 20
    config.NPopF = 20

    result, err := mayfly.Optimize(config)
    if err != nil {
        panic(err)
    }

    fmt.Println("Optimal Hyperparameters:")
    fmt.Printf("  Learning Rate: %.6f\n", result.GlobalBest.Position[0])
    fmt.Printf("  Momentum:      %.6f\n", result.GlobalBest.Position[1])
    fmt.Printf("  Dropout:       %.6f\n", result.GlobalBest.Position[2])
    fmt.Printf("  L2 Reg:        %.6f\n", result.GlobalBest.Position[3])
    fmt.Printf("\nValidation Error: %.6f\n", result.GlobalBest.Cost)
    fmt.Printf("Function Evaluations: %d\n", result.FuncEvalCount)
}
```

## OLCE-MA Parameters

- `UseOLCE`: Enable OLCE-MA variant (default: false)
- `OrthogonalFactor`: Orthogonal learning strength (default: 0.3, range: 0-1)
- `ChaosFactor`: Chaos perturbation strength (default: 0.1, range: 0-1)

## Benefits

- **15-30% improvement** on multimodal functions (Rastrigin, Rosenbrock, Ackley)
- **Better diversity**: Orthogonal learning explores parameter space more systematically
- **Escape stagnation**: Chaotic perturbations help avoid local optima
- **Minimal overhead**: ~12% more function evaluations
- **No tuning needed**: Works well with default parameters

## Performance

### Multimodal Functions

**Rastrigin (D=10, highly multimodal)**:
- Standard MA: 45-60
- OLCE-MA: 15-35 (30%+ improvement)

**Rosenbrock (D=10, narrow valley)**:
- Standard MA: 10-50
- OLCE-MA: 1-10 (significant improvement)

**Ackley (D=10, multimodal)**:
- Standard MA: 2-5
- OLCE-MA: 0.5-2 (50%+ improvement)

### Overhead

OLCE-MA uses approximately 12% more function evaluations:
- Standard MA: ~30,540 evaluations (500 iterations, pop=20)
- OLCE-MA: ~34,200 evaluations (includes orthogonal learning overhead)

## When to Use OLCE-MA

- **Best for**: Multimodal problems with many local optima
- **Excellent on**: High-dimensional problems (10D+)
- **Use when**: Standard MA or DESMA struggle with local optima
- **Ideal for**: Rastrigin, Rosenbrock, Schwefel, Griewank functions

## Parameter Tuning Guide

### Orthogonal Factor

**Default (balanced)**:
```go
config.OrthogonalFactor = 0.3
```
- Good balance between exploration and exploitation
- Recommended for most problems

**More exploration**:
```go
config.OrthogonalFactor = 0.5
```
- Use when: Problem has very high dimensionality
- Use when: Need more systematic parameter space exploration
- Trade-off: More computational overhead

**More exploitation**:
```go
config.OrthogonalFactor = 0.1
```
- Use when: Problem requires fine-tuning near solutions
- Use when: Want minimal overhead
- Trade-off: Less diversity maintenance

### Chaos Factor

**Default (balanced)**:
```go
config.ChaosFactor = 0.1
```
- Provides good local perturbation without disrupting convergence

**Stronger chaos**:
```go
config.ChaosFactor = 0.3
```
- Use when: Need aggressive local optima escape
- Use when: Problem has many deceptive local optima
- Caution: May slow convergence if too high

**Weaker chaos**:
```go
config.ChaosFactor = 0.05
```
- Use when: Solutions need fine refinement
- Use when: Convergence speed is critical
- Trade-off: Less local optima escape capability

## OLCE-MA vs Other Variants

**Choose OLCE-MA when**:
- Problem is highly multimodal (Rastrigin-like)
- High dimensionality (20D+)
- You prioritize solution quality over convergence speed
- Systematic parameter exploration is beneficial

**Choose DESMA instead when**:
- Need simpler adaptive local search
- Function evaluations are cheap
- Want less computational overhead

**Choose EOBBMA instead when**:
- Problem is highly deceptive (Schwefel-like)
- Want simplest parameter tuning
- Heavy-tailed jumps are more effective than chaos

**Choose GSASMA instead when**:
- Need maximum convergence speed
- Simulated annealing fits problem structure
- Prefer hybrid mutation over orthogonal learning

## Algorithm Details

### Orthogonal Learning Application

1. **Select elite males**: Top 20% of population by fitness
2. **Generate orthogonal array**: Systematic parameter combinations
3. **Evaluate combinations**: Test each orthogonal combination
4. **Select best**: Keep best orthogonally-learned solutions

### Chaotic Perturbation Application

1. **After crossover**: Generate offspring from parents
2. **Apply chaos map**: Perturb each dimension with chaotic value
3. **Boundary handling**: Clamp to search space bounds
4. **Evaluate**: Test chaotically-perturbed offspring

## Related Documentation

- [DESMA](desma.md) - Simpler elite-based variant
- [EOBBMA](eobbma.md) - Heavy-tailed exploration alternative
- [Standard MA](standard-ma.md) - Base algorithm
- [Configuration Guide](../api/configuration.md) - Complete parameter reference
